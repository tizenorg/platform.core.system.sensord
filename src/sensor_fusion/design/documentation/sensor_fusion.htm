<html>
<head><title>Sensor Fusion for Tizen Sensor Framework</title></head>

<h1><center>Sensor Fusion for Tizen Sensor Framework</center></h1>

<h2>1. Introduction</h2>

<p>Sensor Fusion is the process of combining the accelerometer,
gyroscope and geo-magnetic sensor in order to generate accurate virtual sensor
outputs such as Orientation, Gravity, Linear Acceleration, etc. Sensor Fusion
is used for extracting individual virtual sensor components from composite
sensor data and/or combining multiple sensor data to create new sensor component
data while compensating for individual sensor errors. Ideally the following
errors would have to be corrected during sensor fusion:-</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Bias: Any non-zero sensor
output when the input is zero</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Scale factor error: error
resulting from aging or manufacturing tolerances</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Nonlinearity: Present in
most sensors to some degree</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Scale factor sign asymmetry:
Often mismatched push-pull amplifiers</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Dead zone: usually due to
mechanical lock-in</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Quantization error: inherent
in all digitized systems</p>

<h2>2. Sensors Used for Sensor Fusion</h2>

<p>Accelerometer Sensor :- Accelerometer data is a combination of linear
acceleration and gravity components. Applications would be interested in using
linear acceleration and gravity sensor data separately. Sensor fusion could be
used to separate linear acceleration and gravity components. Additionally,
accelerometer is used for correcting the roll and pitch orientation measurements
generated from the gyroscope. Using the same, corrected tilt measurement (roll
and pitch) is generated. </p>

<p>Gyroscope Sensor :- It is ideal to determine the orientation of the device,
but has the problem of long term drift in the measured sensor values. Sensor
Fusion could be used to combine Accelerometer, Gyroscope and Geomagnetic (Compass)
sensor data to produce corrected orientation data without drift.</p>

<p>Geo-Magnetic Sensor :- Provides the direction the device is pointed in relation
to the earth's magnetic field. Could be used along with gyroscope angular rotation
along Z axis to produce correct yaw measurement. Geo-Magnetic sensor along with
GPS latitude-longitude measurements could be used to accurately estimate
heading of the device.</p>

<h2>3. Orientation Estimation</h2>

<FIGURE>
<center>
<img src="./diagram/device_orientation.png" width="30%" height="40%">
<FIGCAPTION>Fig. 1. Device Orientation in Euler Angles.</FIGCAPTION>
</center>
</FIGURE>

<p>The rotation of the device along the y-axis is considered as roll (&#934;),
x-axis as pitch (&#920;) and the z-axis as yaw (&#936;) as shown in Fig. 1. The
orientation of the device can be represented either in terms of Euler angles
(roll, pitch, yaw), or in the form of Rotation Matrix, or in the form of
Quaternions. These are different mathematical forms of representing the same
device orientation data. During orientation estimation all these representations
would be used based on requirements and for stability reasons. When the device is
lying flat on the x-y axis, the earth’s gravitational force is observed along the
device z-axis. The reference axis for the device considered in this paper is shown
in Fig. 1. The device reference x and y axis may change for each device, based on
the individual MEMS sensor chip reference axis (that can be obtained from the
datasheet) or the orientation of the sensor chip when it gets integrated into the
device. The equations related to the computation of orientation, gravity and
linear acceleration virtual sensors would have to be modified based on the change
in reference axes.</p>

<FIGURE>
<center>
<img src="./diagram/block_diagram_orientation_estimation.png" width="40%"
height="80%">
<FIGCAPTION>Fig. 2. Quaternion based Sensor Fusion Approach for Orientation
Estimation.</FIGCAPTION>
</center>
</FIGURE>

<p>The overall method for determining orientation of a device based on sensor
fusion is shown in Fig. 2. This paper proposes to improve on the existing approach
[1] to obtain more accurate orientation estimate. The Aiding System is used in sensor
fusion for computing an inaccurate value for device orientation based on the
accelerometer and the magnetometer sensor data. The accelerometer and magnetometer
sensor data is combined using the Triad algorithm explained in [3] to obtain an
inaccurate orientation measure. The problem with the orientation measured using the
aiding system is that the smaller orientation changes of the device are not detected
accurately. The orientation measured using the aiding system is not accurate due to
the effect of gravity on the sensor data. But the aiding system orientation measured
has the advantage that it is not affected by drift. The driving system is used for
computing the orientation of a device using the 3D angular rates measured by the
gyroscope sensor. The gyroscope sensor is accurate in detecting even small
orientation changes but the orientation derived from it are affected due to drift.
The drift in the measured gyroscope orientation is due to the integration of the
noise components present along with angular rates samples measured with the gyroscope.
</p>

<p>The Kalman filtering stage consists of two systems, the time update system where
the current instances of the state vector and prediction error covariance are
estimated based on the measurements obtained from the previous time instance. The
orientation data that is measured using the aiding system and the driving system are
fused during this stage. The second stage of the Kalman filtering process is the
measurement update system, where the estimated state and prediction error covariance
are corrected based on the Kalman gain factor that computed in this stage. The bias
that is estimated during this stage is used to correct the pre-processed gyroscope
sensor data that is given as input to the time update system.</p>

<h3>3.1. Preprocessing of Sensor Data</h3>
<p>The raw sensor data obtained from accelerometer (RAx, RAy, RAz), gyroscope
(RGx, RGy, RGz) and magnetometer (Magx, Magy, Magz) would have to pre-processed
before the sensor fusion process. The raw sensor data obtained from the
accelerometer and gyroscope sensors are affected by static bias, which are the
non-zero sensor data values observed when the device is void of any external forces.
These static bias components on the 3-axes (BAx, BAy, BAz) for accelerometer and
(BGx, BGy, BGz) for gyroscope are removed from the reported sensor values as shown in
(1) and (2). There is no static bias compensation for magnetometer data, as the
sensor measures the deviation of the x-axis relative to the earth's magnetic poles
and it is not possible to determine if the phone is deviating from exact north. The
'i' in the equations below specifies the current time instant, 'i-1' specifies the
previous time instant and a '0' specifies it as an initialization value.</p>

<FIGURE>
<center>
<img src="./equation/equation_1.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_2.png" width="35%" height="5%">
</center>
</FIGURE>

<p>The accelerometer and magnetometer data are normalized based on equations (3)
and (4) to obtain the calibrated accelerometer data (Ax, Ay, Az) and magnetometer
data (Mx, My, Mz).</p>

<FIGURE>
<center>
<img src="./equation/equation_3.png" width="35%" height="9%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_4.png" width="35%" height="9%">
</center>
</FIGURE>

<p>The paper proposes that the gyroscope angular rates (on all 3 axes) before
being processed further are scaled to the range -1 to +1 (for accurate orientation
estimation) by dividing all data received by the maximum possible gyroscope
angular rate value apriori in each axis (5).</p>

<FIGURE>
<center>
<img src="./equation/equation_5.png" width="35%" height="8%">
</center>
</FIGURE>

<p>Based on [1], the dynamically variable Gyroscope Bias (Bx, By, Bz) is computed
using the Kalman filter and provided as feedback to the input and is subtracted
from scaled gyroscope data to obtain the corrected and pre-processed gyroscope
data (Gx, Gy, Gz) given in (6).</p>

<FIGURE>
<center>
<img src="./equation/equation_6.png" width="35%" height="4%">
</center>
</FIGURE>


<h3>3.2. Orientation Computation Based on Aiding System</h3>

<p>The device is placed on a flat surface as shown in the Fig. 1. The gravity
vector in the earth frame for the device (Ae) is given by (0, 0, ±1) since the
effect of gravity is observed on the z-axis reading of the Accelerometer. The sign
assigned is based on whether the z-axis of the accelerometer (when facing up) is
aligned towards gravity or against it. The magnetic field vector in the earth frame
for the device (Me) is given by (0, ±1, 0) since the magnetic north is detected on
the y-axis of the magnetometer. The sign assigned is based on whether the y-axis
of the magnetometer is aligned to earth’s magnetic field or against it. The gravity
vector in the device body frame (Ab) is given by (Ax, Ay, Az), which represents the
calibrated accelerometer sensor data. The magnetic field vector in the device body
frame (Mb) is given by (Mx, My, Mz) which represents the calibrated magnetometer
sensor data.</p>

<p>The orientation of the device is computed based on the aiding system
(accelerometer + magnetometer) data and can be computed using the triad algorithm
[3]. The triad algorithm determines the orientation of the device based on the
gravity and magnetic field vectors obtained in the earth and device body frames.
The following equations represent the triad algorithm using which the orientation
of the device in the form of rotation matrix can be obtained. The symbol × denotes
the cross product of two vectors [8]. Combining the vectors obtained from (7) and
(9) along with the body frame gravity vector (Ab) the intermediate rotation matrix
is obtained for body frame MATb, as shown in (11). Combining the vectors obtained
from (8) and (10) along with the earth frame gravity vector (Ae) the intermediate
rotation matrix is obtained for earth frame MATe, as shown in (12).</p>

<FIGURE>
<center>
<img src="./equation/equation_7.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_8.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_9.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_10.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_11.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_12.png" width="35%" height="3.5%">
</center>
</FIGURE>

<p>Finally the rotation matrix of the device representing the orientation derived
from the aiding system RMaid is computed by multiplying the matrices MATb and MATe
as shown in (13).</p>

<FIGURE>
<center>
<img src="./equation/equation_13_updated.png" width="35%" height="5%">
</center>
</FIGURE>

<p>The device orientation computed in the form of aiding system rotation matrix is
then converted to quaternion representation Qaid (14) using the function
RotMat2Quat() which is explained in [6, 9]. Using quaternions to estimate
orientation overcomes the singularity issues faced when using Euler Angles and at
the same time they are computationally more efficient than using rotation matrix.</p>

<FIGURE>
<center>
<img src="./equation/equation_14.png" width="35%" height="4%">
</center>
</FIGURE>

<h3>3.3. Orientation Computation Based on Driving System</h3>

<p>The sensor data obtained from the driving system (gyroscope) given by (Gx, Gy, Gz)
represents the calibrated angular rotation rate of the device in the 3-axes. Since
the calibrated gyroscope data provides the rate of change of angle in each axis,
the integration of the data acquired in each axis provides the orientation measure
of the device. During integration of the gyroscope data the noise present in each
instance of the measured data gets added up resulting in measured orientation
affected by drift. First the measured gyroscope data is converted to a quaternion
equivalent Qdriv [14, 15]. The initial value for the Qdriv is assigned based on the
computed Qaid initial value (15). The quaternion differential which denotes the
increment in rotation is computed by quaternion based multiplication [7, 9] of Qdriv
with gyroscope sensor data as shown in (16). The symbol &odot; denotes quaternion
based multiplication in the following equations.</p>

<FIGURE>
<center>
<img src="./equation/equation_15.png" width="35%" height="3.5%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_16.png" width="35%" height="6%">
</center>
</FIGURE>

<p>Equation (17) represents the integration of the quaternion difference value with
the driving system quaternion to yield the measured quaternion value for that time
instant [1]. The value 'dt' represents the sampling interval for the gyroscope sensor
and '&Pi;' denotes the mathematical constant and denotes the ratio of a circle's
circumference to its diameter and is approximately equal to 3.14159. The driving
system quaternion is then normalized as shown in equation (18).</p>

<FIGURE>
<center>
<img src="./equation/equation_17.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_18.png" width="35%" height="6%">
</center>
</FIGURE>

<h3>3.4. Time Update System</h3>

<p>The orientation computed using the driving system sensor data is affected due to
the drift (from the integration of angular rates), the orientation computed using
the aiding system sensor data is not accurate (measurements are affected due to gravity).
To compensate for the deficiencies in the measurements of each system, the quaternion
orientation computations are combined using sensor fusion to obtain corrected/more
accurate device orientation data. The Quaternion error Qerr is computed in (19), as
described in [1].</p>

<FIGURE>
<center>
<img src="./equation/equation_19.png" width="35%" height="3.75%">
</center>
</FIGURE>

<p>We introduce the equations (20) and (21) to compute the Euler error from the
quaternion error and then using this Euler error to compensate for drift on the driving
system orientation quaternion. First, the quaternion error Qerr is converted to the Euler
angle representation Eerr based on (20). The quaternion representation of orientation
error is converted to Euler angles representation using the function quat2euler() for
which information can be found in [9].</p>

<FIGURE>
<center>
<img src="./equation/equation_20.png" width="35%" height="6%">
</center>
</FIGURE>

<p>The driving system quaternion is then compensated for the Euler angle error Eerr and
normalized as per (21) and (22) to correct for drift observed in the driving system sensor
data.</p>

<FIGURE>
<center>
<img src="./equation/equation_21.png" width="35%" height="3.75%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_22.png" width="35%" height="6%">
</center>
</FIGURE>

<p>The quaternion Qdriv that is derived in (22) is now compensated for drift and corrected
for dynamic bias as shown in (6). The dynamic bias correction used in (6) is determined
using Kalman filtering as explained below. Based on [1], linear Kalman filtering is used
based on the error state equation, given in (23). The first three elements of the error
state vector are the Eerr values on the 3-axes (&#934;err, &#920;err, &#936;err) and the
next three elements are the bias compensation (Bx, By, Bz) that is mentioned in (6).</p>

<FIGURE>
<center>
<img src="./equation/equation_23.png" width="35%" height="3.75%">
</center>
</FIGURE>

<p>The variance in the gyroscope angular rate measurements denoted by Qwn over a windowed
period is computed in (24). The random driving process noise variance Qwb is computed in
(25), where the noise standard deviation σw and time constant τw are obtained from the
gyroscope sensor datasheet. Based on [1], the overall process noise covariance Q is
derived from the computed Qwn and Qwb as shown in (26).</p>

<FIGURE>
<center>
<img src="./equation/equation_24.png" width="35%" height="8%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_25.png" width="35%" height="8%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_26.png" width="35%" height="6%">
</center>
</FIGURE>

<p>The variance in the aiding system orientation (roll, pitch and yaw) measurements are
used to compute the measurement noise covariance R [1] which is shown in (27). The aiding
system orientation in terms of Euler angles Eaid can be obtained by converting the
quaternion obtained in (14) to its equivalent Euler angles.</p>

<FIGURE>
<center>
<img src="./equation/equation_27.png" width="35%" height="14%">
</center>
</FIGURE>

<p>The equations (28), (29) and (30) are used to determine the transformation matrix F,
apriori state vector error &Delta;x- and prediction covariance P- based in [1].</p>

<FIGURE>
<center>
<img src="./equation/equation_28.png" width="35%" height="15%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_29.png" width="35%" height="3.5%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_30.png" width="35%" height="3.5%">
</center>
</FIGURE>


<h3>3.5. Measurement Update System</h3>
<p>The measurement update system is used to determine the bias value that would be
deducted from the Gyroscope values (6). The equations (31), (32) and (33) are used to
compute the Kalman gain K, aposteriori state error computation &Delta;x+(i) and
aposteriori prediction covariance P+, as shown in [4]. In equation (33), I denotes the
identity matrix H denotes the measurement matrix (identity matrix is used here) and the
apriori prediction covariance estimate P- (33).</p>

<FIGURE>
<center>
<img src="./equation/equation_31.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_32.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_33.png" width="35%" height="4%">
</center>
</FIGURE>

<p>The bias compensation (Bx, By, Bz) obtained from &Delta;x+(i) is used for removing
dynamic bias from the calibrated gyroscope values as shown in (6). The corrected
orientation that is determined using the above sensor fusion method, is obtained from
(22) by using the conversion function quat2euler [9] as shown in (34). This estimated
orientation is used in Section 3 to compute Gravity virtual sensor data.</p>

<FIGURE>
<center>
<img src="./equation/equation_34.png" width="35%" height="4%">
</center>
</FIGURE>

<h3>4. Determination of Gravity and Linear Acceleration</h3>
<p>When a device is subjected to motion in Euclidean space, the 3D accelerometer
data generated from the device is a combination of linear acceleration and gravity
components which are a measure of linear and rotational motion respectively.The
gravity measurements in 3D space are derived from the orientation (pitch, roll
and yaw) measurements that is output from the Kalman filter. The process to compute
gravity and linear acceleration is shown in figure below.

<FIGURE>
<center>
<img src="./diagram/block_diagram_gravity_and_linear_acceleration.png"
width="40%" height="40%">
<FIGCAPTION>Fig. 4. Computation of Gravity and Linear Acceleration.</FIGCAPTION>
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_35.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_36.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_37.png" width="35%" height="4%">
</center>
</FIGURE>

<p>Gravity virtual sensor data provides the measure of the direction in which the
Earth's gravitational force is observed in the device frame of reference. The
orientation of the device decides the measure of the influence of Earth's gravitational
force on the 3-axes of the device. The following equations are used for projecting
the tilt(pitch, roll) of the device on the Earth's gravity axis to determine earth's
gravitational effect on the devices reference axis.</p>

<FIGURE>
<center>
<img src="./diagram/orientation_effect_on_gravity.png">
<FIGCAPTION>Fig. 5. Effect of Device Orientation on Gravity.</FIGCAPTION>
</center>
</FIGURE>

<p>When the device tilt values (pitch,roll) are changed from (0,0) to (0,&Pi;/2),
phone is rotated around y-axis, the x-axis gets aligned to earth's gravitational field
after rotation instead of the z-axis. When this rotation is applied to the equations
given above, the values (GRx,GRy,GRz) are converted from (0,0,G) to (G,0,0) due to the
shift in the axis which experiences the gravitational field (G is measure of Earth's
gravity).</p>

<h2>Determination of Linear Acceleration</h2>

<p>Accurate linear acceleration data are calculated by subtracting gravity components
from the 3-axes calibrated accelerometer data as shown in (38), (39) and (40). As shown
in (38) the rotation of the device on y-axis (GRy) reflects on the accelerometer x-axis
sensor data (Ax). The linear acceleration measurement on x-axis (LAx) directly
corresponds to the accelerometer x-axis sensor data (Ax) meaning linear motion along
x-axis is directly measured on the accelerometer x-axis.</p>


<FIGURE>
<center>
<img src="./equation/equation_38.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_39.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_40.png" width="35%" height="4%">
</center>
</FIGURE>

<h1>References</h1>

<p>[1] Gebre-Egziabher, H., Rhayward, R. C. &amp; Powell, J. D. Design of Multi-Sensor
Attitude Determination Systems. IEEE Transactions on AESS, 40(2), 627 - 649 (2004)</p>

<p>[2] Abyarjoo1, et al. Implementing a Sensor Fusion Algorithm for 3D Orientation
Detection with Inertial/Magnetic Sensors. The International Joint Conferences on CISSE
(2012)</p>

<p>[3] Marcard, T. V., Design and Implementation of an attitude estimation system to
control orthopedic components. Chalmers University. Master thesis published on the
university link http://publications.lib.chalmers.se/records/fulltext/125985.pdf(2010)</p>

<p>[4] Welch, G. &amp; Bishop, G. An introduction to the Kalman Filter. SIGGRAPH 2001</p>

<p>[5] Grewal, M. S., et al. Global Positioning Systems, Inertial Navigation, and
Integration (John Wiley &amp; Sons., 2001)</p>

<p>[6] Greenberg, M. J., Euclidean and non-Euclidean geometry: Development and History
(Freeman, 1980)</p>

<p>[7] Conway, J. H &amp; Smith, D. A. On Quaternions and Octonions: Their Geometry,
Arithmetic, and Symmetry (Peters, 2003)</p>

<p>[8] Zill, D. G &amp; Cullen, M. R. "Definition 7.4: Cross product of two vectors".
Advanced engineering mathematics (Jones &amp; Bartlett Learning, 2006)</p>

<p>[9] NASA Mission Planning and Analysis Division. Euler Angles, Quaternions, and
Transformation Matrices. Document Link-
http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19770024290.pdf. Last Accessed
- July 2014.</p>

<p>[10] Android Sensor Fusion Library. Source code link:
https://android.googlesource.com/platform/frameworks/native/+/b398927/services/sensorservice/.
Last Accessed - July 2014.</p>

<p>[11] GNU Octave High Level Interpreter. Software Link: http://www.gnu.org/software/octave/.
Last Accessed - July 2014.</p>

<p>[12] Tizen OS - https://www.tizen.org/. Last Accessed - July 2014.</p>

<p>[13] Marins, J. L., et al. An Extended Kalman Filter for Quaternion-Based Orientation
Estimation Using MARG Sensors. IEEE/RSJ International Conference on Intelligent Robots and
Systems (2001)</p>

<p>[14] Favre, J., et al. Quaternion-based fusion of gyroscopes and accelerometers to improve
3D angle measurement. ELECTRONICS LETTERS, 25th May 2006, Vol. 42 No. 11</p>

<p>[15] Sabatini, A.M., Quaternion based attitude estimation algorithm applied to signals
from body-mounted gyroscopes.  ELECTRONICS LETTERS, 13th May 2004, Vol. 40 No. 10</p>

</html>
