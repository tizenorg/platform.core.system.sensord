<html>
<head><title>Sensor Fusion for Tizen Sensor Framework</title></head>

<h1><center>Sensor Fusion for Tizen Sensor Framework</center></h1>

<h2>1. Introduction</h2>

<p>Sensor Fusion is the process of combining the accelerometer,
gyroscope and geo-magnetic sensor in order to generate accurate virtual sensor
outputs such as Orientation, Gravity, Linear Acceleration, etc. Sensor Fusion
is used for extracting individual virtual sensor components from composite
sensor data and/or combining multiple sensor data to create new sensor component
data while compensating for individual sensor errors. Ideally the following
errors would have to be corrected during sensor fusion:-</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Bias: Any non-zero sensor
output when the input is zero</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Scale factor error: error
resulting from aging or manufacturing tolerances</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Nonlinearity: Present in
most sensors to some degree</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Scale factor sign asymmetry:
Often mismatched push-pull amplifiers</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Dead zone: usually due to
mechanical lock-in</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Quantization error: inherent
in all digitized systems</p>

<h2>2. Sensors Used for Sensor Fusion</h2>

<p>Accelerometer Sensor :- Accelerometer data is a combination of linear
acceleration and gravity components. Applications would be interested in using
linear acceleration and gravity sensor data separately. Sensor fusion could be
used to separate linear acceleration and gravity components. Additionally,
accelerometer is used for correcting the roll and pitch orientation measurements
generated from the gyroscope. Using the same, corrected tilt measurement (roll
and pitch) is generated. </p>

<p>Gyroscope Sensor :- It is ideal to determine the orientation of the device,
but has the problem of long term drift in the measured sensor values. Sensor
Fusion could be used to combine Accelerometer, Gyroscope and Geomagnetic (Compass)
sensor data to produce corrected orientation data without drift.</p>

<p>Geo-Magnetic Sensor :- Provides the direction the device is pointed in relation
to the earth's magnetic field. Could be used along with gyroscope angular rotation
along Z axis to produce correct yaw measurement. Geo-Magnetic sensor along with
GPS latitude-longitude measurements could be used to accurately estimate
heading of the device.</p>

<h2>3. Orientation Estimation</h2>

<FIGURE>
<center>
<img src="./diagram/device_orientation.png" width="30%" height="40%">
<FIGCAPTION>Fig. 1. Device Orientation in Euler Angles.</FIGCAPTION>
</center>
</FIGURE>

<p>The rotation of the device along the y-axis is considered as roll (&#934;),
x-axis as pitch (&#920;) and the z-axis as yaw (&#936;) as shown in Fig. 1. The
orientation of the device can be represented either in terms of Euler angles
(roll, pitch, yaw), or in the form of Rotation Matrix, or in the form of
Quaternions. These are different mathematical forms of representing the same
device orientation data. During orientation estimation all these representations
would be used based on requirements and for stability reasons. When the device is
lying flat on the x-y axis, the earth’s gravitational force is observed along the
device z-axis. The reference axis for the device considered in this paper is shown
in Fig. 1. The device reference x and y axis may change for each device, based on
the individual MEMS sensor chip reference axis (that can be obtained from the
datasheet) or the orientation of the sensor chip when it gets integrated into the
device. The equations related to the computation of orientation, gravity and
linear acceleration virtual sensors would have to be modified based on the change
in reference axes.</p>

<FIGURE>
<center>
<img src="./diagram/block_diagram_orientation_estimation.png" width="40%"
height="80%">
<FIGCAPTION>Fig. 2. Quaternion based Sensor Fusion Approach for Orientation
Estimation.</FIGCAPTION>
</center>
</FIGURE>

<p>The overall method for determining orientation of a device based on sensor
fusion is shown in Fig. 2. This paper proposes to improve on the existing approach
[1] to obtain more accurate orientation estimate. The Aiding System is used in sensor
fusion for computing an inaccurate value for device orientation based on the
accelerometer and the magnetometer sensor data. The accelerometer and magnetometer
sensor data is combined using the Triad algorithm explained in [3] to obtain an
inaccurate orientation measure. The problem with the orientation measured using the
aiding system is that the smaller orientation changes of the device are not detected
accurately. The orientation measured using the aiding system is not accurate due to
the effect of gravity on the sensor data. But the aiding system orientation measured
has the advantage that it is not affected by drift. The driving system is used for
computing the orientation of a device using the 3D angular rates measured by the
gyroscope sensor. The gyroscope sensor is accurate in detecting even small
orientation changes but the orientation derived from it are affected due to drift.
The drift in the measured gyroscope orientation is due to the integration of the
noise components present along with angular rates samples measured with the gyroscope.
</p>

<p>The Kalman filtering stage consists of two systems, the time update system where
the current instances of the state vector and prediction error covariance are
estimated based on the measurements obtained from the previous time instance. The
orientation data that is measured using the aiding system and the driving system are
fused during this stage. The second stage of the Kalman filtering process is the
measurement update system, where the estimated state and prediction error covariance
are corrected based on the Kalman gain factor that computed in this stage. The bias
that is estimated during this stage is used to correct the pre-processed gyroscope
sensor data that is given as input to the time update system.</p>

<h3>3.1. Preprocessing of Sensor Data</h3>

<FIGURE>
<center>
<img src="./equation/equation_1.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_2.png" width="35%" height="5%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_3.png" width="35%" height="9%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_4.png" width="35%" height="9%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_5.png" width="35%" height="8%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_6.png" width="35%" height="4%">
</center>
</FIGURE>


<h3>3.2. Orientation Computation Based on Aiding System</h3>

<FIGURE>
<center>
<img src="./equation/equation_7.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_8.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_9.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_10.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_11.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_12.png" width="35%" height="3.5%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_13.png" width="35%" height="3%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_14.png" width="35%" height="4%">
</center>
</FIGURE>

<h3>3.3. Orientation Computation Based on Driving System</h3>

<FIGURE>
<center>
<img src="./equation/equation_15.png" width="35%" height="3.5%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_16.png" width="35%" height="6%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_17.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_18.png" width="35%" height="6%">
</center>
</FIGURE>

<h3>3.4. Time Update System</h3>

<FIGURE>
<center>
<img src="./equation/equation_19.png" width="35%" height="3.75%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_20.png" width="35%" height="6%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_21.png" width="35%" height="3.75%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_22.png" width="35%" height="6%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_23.png" width="35%" height="3.75%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_24.png" width="35%" height="8%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_25.png" width="35%" height="8%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_26.png" width="35%" height="6%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_27.png" width="35%" height="14%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_28.png" width="35%" height="15%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_29.png" width="35%" height="3.5%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_30.png" width="35%" height="3.5%">
</center>
</FIGURE>


<h3>3.5. Measurement Update System</h3>

<FIGURE>
<center>
<img src="./equation/equation_31.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_32.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_33.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_34.png" width="35%" height="4%">
</center>
</FIGURE>

<h3>4. Determination of Gravity and Linear Acceleration</h3>
<p>When a device is subjected to motion in Euclidean space, the 3D accelerometer
data generated from the device is a combination of linear acceleration and gravity
components which are a measure of linear and rotational motion respectively.The
gravity measurements in 3D space are derived from the orientation (pitch, roll
and yaw) measurements that is output from the Kalman filter. The process to compute
gravity and linear acceleration is shown in figure below.

<FIGURE>
<center>
<img src="./diagram/block_diagram_gravity_and_linear_acceleration.png"
width="40%" height="40%">
<FIGCAPTION>Fig. 4. Computation of Gravity and Linear Acceleration.</FIGCAPTION>
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_35.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_36.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_37.png" width="35%" height="4%">
</center>
</FIGURE>

<p>Gravity virtual sensor data provides the measure of the direction in which the
Earth's gravitational force is observed in the device frame of reference. The
orientation of the device decides the measure of the influence of Earth's gravitational
force on the 3-axes of the device. The following equations are used for projecting
the tilt(pitch, roll) of the device on the Earth's gravity axis to determine earth's
gravitational effect on the devices reference axis.</p>

<FIGURE>
<center>
<img src="./diagram/orientation_effect_on_gravity.png">
<FIGCAPTION>Fig. 5. Effect of Device Orientation on Gravity.</FIGCAPTION>
</center>
</FIGURE>

<p>When the device tilt values (pitch,roll) are changed from (0,0) to (0,&Pi;/2),
phone is rotated around x-axis, the y-axis gets aligned to earth's gravitational field
after rotation instead of the z-axis.  When this rotation is applied to the equations
given above, the values (GRx,GRy,GRz) are converted from (0,0,G) to (0,G,0) due to the
shift in the axis which experiences the gravitational field (G is measure of Earth's
gravity).</p>

<h2>Determination of Linear Acceleration</h2>

<p>Linear Acceleration virtual sensor data provides the measure of the acceleration of
a device after removing the Gravity components on the 3-axes. Accurate linear
acceleration data are calculated by subtracting gravity components from the 3-axes
calibrated accelerometer data.</p>


<FIGURE>
<center>
<img src="./equation/equation_38.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_39.png" width="35%" height="4%">
</center>
</FIGURE>

<FIGURE>
<center>
<img src="./equation/equation_40.png" width="35%" height="4%">
</center>
</FIGURE>

<h1>References</h1>

<p>[1] Gebre-Egziabher, H., Rhayward, R. C. &amp; Powell, J. D. Design
of Multi-Sensor Attitude Determination Systems. IEEE Transactions on
Aerospace and Electronic Systems, (Volume: 40, Issue: 2), 627 - 649 (2004)</p>

<p>[2] Abyarjoo1, F., Barreto1, A., Cofino1, J. &amp; Ortega, F. R., Implementing
a Sensor Fusion Algorithm for 3D Orientation Detection with Inertial/Magnetic
Sensors. The International Joint Conferences on CISSE (2012)</p>

<p>[3] Marcard, T. V., Design and Implementation of an attitude estimation system to
control orthopedic components. Chalmers University. Master thesis published on
the university link http://publications.lib.chalmers.se/records/fulltext/125985.pdf(2010)</p>

<p>[4] Welch, G., Bishop, G. An introduction to the Kalman Filter: SIGGRAPH (2001)</p>

<p>[5] Grewal, M. S., Weill, L. R., Andrews, A. P., Global Positioning Systems,
Inertial Navigation, and Integration (John Wiley &amp; Sons., 2001)</p>

</html>
